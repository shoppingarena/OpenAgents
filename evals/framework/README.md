# Evaluation Framework (Technical)

Core framework for evaluating agent behavior. For user documentation, see [../README.md](../README.md).

## Architecture

```
framework/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ sdk/                 # Test execution
â”‚   â”‚   â”œâ”€â”€ test-runner.ts   # Main orchestrator
â”‚   â”‚   â”œâ”€â”€ test-executor.ts # Executes individual tests
â”‚   â”‚   â”œâ”€â”€ client-manager.ts
â”‚   â”‚   â””â”€â”€ event-stream-handler.ts
â”‚   â”œâ”€â”€ evaluators/          # Rule validators
â”‚   â”‚   â”œâ”€â”€ base-evaluator.ts
â”‚   â”‚   â”œâ”€â”€ approval-gate-evaluator.ts
â”‚   â”‚   â”œâ”€â”€ context-loading-evaluator.ts
â”‚   â”‚   â”œâ”€â”€ execution-balance-evaluator.ts
â”‚   â”‚   â”œâ”€â”€ tool-usage-evaluator.ts
â”‚   â”‚   â”œâ”€â”€ behavior-evaluator.ts
â”‚   â”‚   â”œâ”€â”€ delegation-evaluator.ts
â”‚   â”‚   â”œâ”€â”€ stop-on-failure-evaluator.ts
â”‚   â”‚   â””â”€â”€ performance-metrics-evaluator.ts  # NEW
â”‚   â”œâ”€â”€ logging/             # Multi-agent logging (NEW)
â”‚   â”‚   â”œâ”€â”€ types.ts
â”‚   â”‚   â”œâ”€â”€ session-tracker.ts
â”‚   â”‚   â”œâ”€â”€ logger.ts
â”‚   â”‚   â”œâ”€â”€ formatters.ts
â”‚   â”‚   â”œâ”€â”€ index.ts
â”‚   â”‚   â””â”€â”€ __tests__/       # 37 unit tests
â”‚   â”œâ”€â”€ collector/           # Session data
â”‚   â”‚   â”œâ”€â”€ session-reader.ts
â”‚   â”‚   â””â”€â”€ timeline-builder.ts
â”‚   â””â”€â”€ types/
â”‚       â””â”€â”€ index.ts
â””â”€â”€ package.json
```

## Evaluators

### approval-gate
Checks that approval is requested before risky operations (bash, write, edit, task).

### context-loading
Verifies context files are loaded before acting on tasks.

**NEW:** Supports explicit context file specification via `expectedContextFiles` in test YAML.
- Auto-detect mode: Infers expected files from user message keywords
- Explicit mode: Uses files specified in `behavior.expectedContextFiles`

### execution-balance
Ensures read operations happen before write operations.

### tool-usage
Validates dedicated tools are used instead of bash antipatterns.

### behavior
Checks expected tools are used and forbidden tools are avoided.

### delegation
Validates complex tasks are delegated to subagents.

### stop-on-failure
Ensures agent stops on errors instead of auto-fixing.

### performance-metrics (NEW)
Collects performance data for analysis:
- Total test duration
- Tool latencies (avg, min, max per tool)
- LLM inference time estimation
- Idle time between events
- Event distribution

Always passes - used for metrics collection only.

## Multi-Agent Logging (NEW)

The framework now includes comprehensive multi-agent logging that tracks delegation hierarchies in real-time.

### Features
- **Visual hierarchy** - Box characters and indentation show parent-child relationships (debug mode)
- **Session tracking** - Tracks all sessions (parent, child, grandchild, etc.)
- **Real-time capture** - Hooks into SDK event stream for live updates
- **Non-verbose mode** - Shows child agent execution in normal mode without full debug output
- **Verbose mode** - Full delegation hierarchy with `--debug` flag

### Usage
```bash
# Non-verbose mode (default) - shows child agent completion
npm run eval:sdk -- --agent=openagent --pattern="**/test.yaml"

# Verbose mode (debug) - shows full delegation hierarchy
npm run eval:sdk -- --agent=openagent --pattern="**/test.yaml" --debug
```

### Example Output (Non-Verbose Mode)
```
Running tests...

   âœ“ Child agent completed (OpenAgent, 2.9s)

Running evaluator: approval-gate...
```

### Example Output (Verbose Mode - Debug)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ¯ PARENT: OpenAgent (ses_xxx...)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  ğŸ”§ TOOL: task
     â”œâ”€ subagent: simple-responder
     â””â”€ Creating child session...

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ ğŸ¯ CHILD: simple-responder (ses_yyy...)                    â”‚
  â”‚    Parent: ses_xxx...                                      â”‚
  â”‚    Depth: 1                                                â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    ğŸ¤– Agent: AWESOME TESTING
  âœ… CHILD COMPLETE (2.9s)

âœ… PARENT COMPLETE (20.9s)
```

See [src/logging/README.md](src/logging/README.md) for API documentation.

## Adding an Evaluator

1. Create `src/evaluators/my-evaluator.ts`:

```typescript
import { BaseEvaluator } from './base-evaluator.js';
import { TimelineEvent, SessionInfo, EvaluationResult } from '../types/index.js';

export class MyEvaluator extends BaseEvaluator {
  name = 'my-evaluator';
  description = 'What this evaluator checks';

  async evaluate(timeline: TimelineEvent[], sessionInfo: SessionInfo): Promise<EvaluationResult> {
    const checks = [];
    const violations = [];
    const evidence = [];

    // Your evaluation logic here
    const toolCalls = this.getToolCalls(timeline);
    
    // Example check
    const passed = toolCalls.length > 0;
    checks.push({
      name: 'has-tool-calls',
      passed,
      weight: 100,
      evidence: [this.createEvidence('tool-count', `Found ${toolCalls.length} tool calls`, {})]
    });

    if (!passed) {
      violations.push(this.createViolation(
        'no-tool-calls',
        'error',
        'No tool calls found',
        Date.now(),
        {}
      ));
    }

    return this.buildResult(this.name, checks, violations, evidence, {});
  }
}
```

2. Register in `test-runner.ts`:

```typescript
import { MyEvaluator } from '../evaluators/my-evaluator.js';

// In setupEvaluators():
this.evaluatorRunner = new EvaluatorRunner({
  evaluators: [
    // ... existing evaluators
    new MyEvaluator(),
  ],
});
```

3. Add to test schema in `test-case-schema.ts`:

```typescript
export const ExpectedViolationSchema = z.object({
  rule: z.enum([
    // ... existing rules
    'my-evaluator',
  ]),
  // ...
});
```

## Development

```bash
# Install
npm install

# Build
npm run build

# Run tests
npm test

# Run SDK tests
npm run eval:sdk -- --agent=openagent --pattern="**/golden/*.yaml"
```

## Key Types

```typescript
interface TimelineEvent {
  timestamp: number;
  type: 'user_message' | 'assistant_message' | 'tool_call' | 'text';
  data: any;
}

interface EvaluationResult {
  evaluator: string;
  passed: boolean;
  score: number;
  violations: Violation[];
  evidence: Evidence[];
  checks: Check[];
}

interface Violation {
  type: string;
  severity: 'error' | 'warning' | 'info';
  message: string;
  timestamp: number;
  evidence?: any;
}
```

## Base Evaluator Helpers

```typescript
// Get all tool calls
const toolCalls = this.getToolCalls(timeline);

// Get specific tool calls
const bashCalls = this.getToolCallsByName(timeline, 'bash');

// Get assistant messages
const messages = this.getAssistantMessages(timeline);

// Get read tools (read, glob, grep, list)
const reads = this.getReadTools(timeline);

// Get execution tools (bash, write, edit, task)
const executions = this.getExecutionTools(timeline);

// Create violation
this.createViolation(type, severity, message, timestamp, evidence);

// Create evidence
this.createEvidence(type, description, data, timestamp?);

// Build result
this.buildResult(name, checks, violations, evidence, metadata);
```
